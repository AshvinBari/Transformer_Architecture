# Transformer_Architecture 
![image](https://github.com/user-attachments/assets/25914410-c570-43d1-b973-ce32f6099b92)
# ğŸš€ Transformer Architecture - From Scratch

This repository demonstrates a **Transformer model** implemented from scratch in **PyTorch**. It is a step-by-step, beginner-friendly guide to understanding how attention and encoder layers are built without using `torch.nn.Transformer`.


---

## ğŸ“˜ Notebook

- [ğŸ”— Transformer_(1).ipynb](https://github.com/AshvinBari/Transformer_Architecture/blob/main/Transformer_(1).ipynb)

The notebook includes:
- Token embedding
- Positional encoding
- Self-attention mechanism
- Feed-forward neural network
- Transformer encoder layer
- Layer normalization and residuals
- Input/output visualization

---

## ğŸ§± Architecture Components

| Component             | Description |
|-----------------------|-------------|
| ğŸ”  Embedding Layer    | Maps tokens to vectors |
| ğŸ“ Positional Encoding | Adds sequence order |
| ğŸ§  Multi-Head Attention | Attends to token relationships |
| ğŸ” Residual Connection | Stabilizes gradient flow |
| ğŸ“ Layer Normalization | Improves convergence |
| ğŸ§® FeedForward Network | Adds non-linearity and depth |

---

## ğŸ› ï¸ Tech Stack

- Python 3.10+
- PyTorch
- NumPy
- Jupyter Notebook

---

## â–¶ï¸ Run Instructions

```bash
git clone https://github.com/AshvinBari/Transformer_Architecture.git
cd Transformer_Architecture
jupyter notebook Transformer_(1).ipynb
```
---

## ğŸ“š References

- [Attention is All You Need â€“ Vaswani et al.](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer â€“ Jay Alammar](https://jalammar.github.io/illustrated-transformer/)
- [PyTorch Documentation â€“ Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)

---

## âœ¨ Author

**Ashvin Bari**  
ğŸ‘¨â€ğŸ’» AI Engineer | ML Developer | Educator  
ğŸ”— [GitHub](https://github.com/AshvinBari) â€¢ [LinkedIn](https://www.linkedin.com/in/ashvinbari/)

---

## ğŸ·ï¸ Tags

`#Transformer` `#PyTorch` `#Attention` `#DeepLearning` `#MachineLearning` `#NLP` `#FromScratch`
