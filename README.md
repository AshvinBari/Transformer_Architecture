# Transformer_Architecture 
![image](https://github.com/user-attachments/assets/25914410-c570-43d1-b973-ce32f6099b92)
# 🚀 Transformer Architecture - From Scratch

This repository demonstrates a **Transformer model** implemented from scratch in **PyTorch**. It is a step-by-step, beginner-friendly guide to understanding how attention and encoder layers are built without using `torch.nn.Transformer`.


---

## 📘 Notebook

- [🔗 Transformer_(1).ipynb](https://github.com/AshvinBari/Transformer_Architecture/blob/main/Transformer_(1).ipynb)

The notebook includes:
- Token embedding
- Positional encoding
- Self-attention mechanism
- Feed-forward neural network
- Transformer encoder layer
- Layer normalization and residuals
- Input/output visualization

---

## 🧱 Architecture Components

| Component             | Description |
|-----------------------|-------------|
| 🔠 Embedding Layer    | Maps tokens to vectors |
| 📍 Positional Encoding | Adds sequence order |
| 🧠 Multi-Head Attention | Attends to token relationships |
| 🔁 Residual Connection | Stabilizes gradient flow |
| 📏 Layer Normalization | Improves convergence |
| 🧮 FeedForward Network | Adds non-linearity and depth |

---

## 🛠️ Tech Stack

- Python 3.10+
- PyTorch
- NumPy
- Jupyter Notebook

---

## ▶️ Run Instructions

```bash
git clone https://github.com/AshvinBari/Transformer_Architecture.git
cd Transformer_Architecture
jupyter notebook Transformer_(1).ipynb
```
---

## 📚 References

- [Attention is All You Need – Vaswani et al.](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer – Jay Alammar](https://jalammar.github.io/illustrated-transformer/)
- [PyTorch Documentation – Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)

---

## ✨ Author

**Ashvin Bari**  
👨‍💻 AI Engineer | ML Developer | Educator  
🔗 [GitHub](https://github.com/AshvinBari) • [LinkedIn](https://www.linkedin.com/in/ashvinbari/)

---

## 🏷️ Tags

`#Transformer` `#PyTorch` `#Attention` `#DeepLearning` `#MachineLearning` `#NLP` `#FromScratch`
